{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "from resnet import ResNet18\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from resnet_fsl_aytin import *\n",
    "\n",
    "# load dataset\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "Y_train = Y_train.astype('float32')\n",
    "Y_test = Y_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplets(x, y, num_triplets):\n",
    "    triplets = []\n",
    "    for _ in range(num_triplets):\n",
    "        # Choose a random anchor index\n",
    "        idx_a = np.random.randint(0, len(y))\n",
    "        \n",
    "        # Get positive and negative indices for this anchor \n",
    "        pos_idx = np.where(y == y[idx_a])[0]\n",
    "        neg_idx = np.where(y != y[idx_a])[0]\n",
    "        \n",
    "        # Select a random positive and negative index\n",
    "        idx_p = np.random.choice(pos_idx)\n",
    "        idx_n = np.random.choice(neg_idx)\n",
    "\n",
    "        triplet = (x[idx_a], x[idx_p], x[idx_n])\n",
    "        triplets.append(triplet)\n",
    "    \n",
    "    return np.array(triplets)\n",
    "\n",
    "triplets = create_triplets(X_train, Y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling MyConv2D.call().\n\n\u001b[1mInput 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (28, 28)\u001b[0m\n\nArguments received by MyConv2D.call():\n  • in_x=tf.Tensor(shape=(28, 28), dtype=float32)\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39mtriplet_loss)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtriplets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtriplets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/Few-Shot-Learning/resnet_fsl_aytin.py:124\u001b[0m, in \u001b[0;36mTripletModel.call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    123\u001b[0m     anchor, positive, negative \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], inputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m], inputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m--> 124\u001b[0m     anchor_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     positive_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model(positive, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[1;32m    126\u001b[0m     negative_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model(negative, training\u001b[38;5;241m=\u001b[39mtraining)\n",
      "File \u001b[0;32m~/Documents/Few-Shot-Learning/resnet_fsl_aytin.py:96\u001b[0m, in \u001b[0;36mResNet18.call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 96\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_pool(x)\n\u001b[1;32m     99\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_1_1(x, training\u001b[38;5;241m=\u001b[39mtraining)\n",
      "File \u001b[0;32m~/Documents/Few-Shot-Learning/resnet_fsl_aytin.py:32\u001b[0m, in \u001b[0;36mMyConv2D.call\u001b[0;34m(self, in_x, training)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    Call method of the custom layer. it applies the convolution and batch normalization to the input data.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling MyConv2D.call().\n\n\u001b[1mInput 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (28, 28)\u001b[0m\n\nArguments received by MyConv2D.call():\n  • in_x=tf.Tensor(shape=(28, 28), dtype=float32)\n  • training=True"
     ]
    }
   ],
   "source": [
    "from resnet_fsl_aytin import TripletModel\n",
    "from resnet_fsl_aytin import ResNet18\n",
    "\n",
    "embedding = ResNet18()\n",
    "\n",
    "# Create a model\n",
    "model = TripletModel(embedding)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(0.001), loss=triplet_loss)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(triplets.reshape(-1, 28, 28, 1), np.zeros((len(triplets), 1)), epochs=10, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import resnet\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_embedding_module(imageSize):\n",
    "    # construct the input layer and pass the inputs through a\n",
    "    # pre-processing layer\n",
    "    inputs = keras.Input(imageSize + (3,))\n",
    "    x = resnet.preprocess_input(inputs)\n",
    "    \n",
    "    # fetch the pre-trained resnet 50 model and freeze the weights\n",
    "    baseCnn = resnet.ResNet50(weights=\"imagenet\", include_top=False)\n",
    "    baseCnn.trainable=False\n",
    "    \n",
    "    # pass the pre-processed inputs through the base cnn and get the\n",
    "    # extracted features from the inputs\n",
    "    extractedFeatures = baseCnn(x)\n",
    "    # pass the extracted features through a number of trainable layers\n",
    "    x = layers.GlobalAveragePooling2D()(extractedFeatures)\n",
    "    x = layers.Dense(units=1024, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(units=256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(units=128)(x)\n",
    "    # build the embedding model and return it\n",
    "    embedding = keras.Model(inputs, outputs, name=\"embedding\")\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def get_siamese_network(imageSize, embeddingModel):\n",
    "    # build the anchor, positive and negative input layer\n",
    "    anchorInput = keras.Input(name=\"anchor\", shape=imageSize + (3,))\n",
    "    positiveInput = keras.Input(name=\"positive\", shape=imageSize + (3,))\n",
    "    negativeInput = keras.Input(name=\"negative\", shape=imageSize + (3,))\n",
    "    # embed the anchor, positive and negative images\n",
    "    anchorEmbedding = embeddingModel(anchorInput)\n",
    "    positiveEmbedding = embeddingModel(positiveInput)\n",
    "    negativeEmbedding = embeddingModel(negativeInput)\n",
    "    # build the siamese network and return it\n",
    "    siamese_network = keras.Model(\n",
    "        inputs=[anchorInput, positiveInput, negativeInput],\n",
    "        outputs=[anchorEmbedding, positiveEmbedding, negativeEmbedding]\n",
    "    )\n",
    "    return siamese_network\n",
    "\n",
    "class SiameseModel(keras.Model):\n",
    "    def __init__(self, siameseNetwork, margin, lossTracker):\n",
    "        super().__init__()\n",
    "        self.siameseNetwork = siameseNetwork\n",
    "        self.margin = margin\n",
    "        self.lossTracker = lossTracker\n",
    "    def _compute_distance(self, inputs):\n",
    "        print(inputs)\n",
    "        (anchor, positive, negative) = inputs\n",
    "\n",
    "        # embed the images using the siamese network\n",
    "        embeddings = self.siameseNetwork((anchor, positive, negative))\n",
    "        anchorEmbedding = embeddings[0]\n",
    "        positiveEmbedding = embeddings[1]\n",
    "        negativeEmbedding = embeddings[2]\n",
    "\n",
    "        # calculate the anchor to positive and negative distance\n",
    "        apDistance = tf.reduce_sum(\n",
    "            tf.square(anchorEmbedding - positiveEmbedding), axis=-1\n",
    "        )\n",
    "        anDistance = tf.reduce_sum(\n",
    "            tf.square(anchorEmbedding - negativeEmbedding), axis=-1\n",
    "        )\n",
    "        \n",
    "        # return the distances\n",
    "        return (apDistance, anDistance)\n",
    "    def _compute_loss(self, apDistance, anDistance):\n",
    "        loss = apDistance - anDistance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "    def call(self, inputs):\n",
    "        # compute the distance between the anchor and positive,\n",
    "        # negative images\n",
    "        (apDistance, anDistance) = self._compute_distance(inputs)\n",
    "        return (apDistance, anDistance)\n",
    "    def train_step(self, inputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # compute the distance between the anchor and positive,\n",
    "            # negative images\n",
    "            (apDistance, anDistance) = self._compute_distance(inputs)\n",
    "            # calculate the loss of the siamese network\n",
    "            loss = self._compute_loss(apDistance, anDistance)\n",
    "        # compute the gradients and optimize the model\n",
    "        gradients = tape.gradient(\n",
    "            loss,\n",
    "            self.siameseNetwork.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siameseNetwork.trainable_variables)\n",
    "        )\n",
    "        # update the metrics and return the loss\n",
    "        self.lossTracker.update_state(loss)\n",
    "        return {\"loss\": self.lossTracker.result()}\n",
    "    def test_step(self, inputs):\n",
    "        # compute the distance between the anchor and positive,\n",
    "        # negative images\n",
    "        (apDistance, anDistance) = self._compute_distance(inputs)\n",
    "        # calculate the loss of the siamese network\n",
    "        loss = self._compute_loss(apDistance, anDistance)\n",
    "        \n",
    "        # update the metrics and return the loss\n",
    "        self.lossTracker.update_state(loss)\n",
    "        return {\"loss\": self.lossTracker.result()}\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.lossTracker]\n",
    "    \n",
    "\n",
    "# # initialize the image size and the margin\n",
    "# imageSize = (28, 28)\n",
    "\n",
    "# # get the embedding model\n",
    "# embeddingModel = get_embedding_module(imageSize)\n",
    "\n",
    "# # get the siamese network\n",
    "# siameseNetwork = get_siamese_network(imageSize, embeddingModel)\n",
    "\n",
    "# # initialize the margin and the loss tracker\n",
    "# margin = 1\n",
    "# lossTracker = tf.metrics.Mean(name=\"loss\")\n",
    "\n",
    "# # build the siamese model\n",
    "# model = SiameseModel(siameseNetwork, margin, lossTracker)\n",
    "\n",
    "# # compile the model\n",
    "# model.compile(optimizer=keras.optimizers.Adam(0.001))\n",
    "\n",
    "# # train the model\n",
    "# for _ in range(10):\n",
    "#     model.train_step(triplets[_])\n",
    "# # model.train_step(triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# send only 1 data at a time \u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 27\u001b[0m     \u001b[43msiamese_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     31\u001b[0m history \u001b[38;5;241m=\u001b[39m siamese_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     32\u001b[0m     train_data,\n\u001b[1;32m     33\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     34\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     35\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mtest_data\n\u001b[1;32m     36\u001b[0m )\n",
      "Cell \u001b[0;32mIn[5], line 90\u001b[0m, in \u001b[0;36mSiameseModel.train_step\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;66;03m# compute the distance between the anchor and positive,\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;66;03m# negative images\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m         (apDistance, anDistance) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;66;03m# calculate the loss of the siamese network\u001b[39;00m\n\u001b[1;32m     92\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss(apDistance, anDistance)\n",
      "Cell \u001b[0;32mIn[5], line 59\u001b[0m, in \u001b[0;36mSiameseModel._compute_distance\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_distance\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(inputs)\n\u001b[0;32m---> 59\u001b[0m     (anchor, positive, negative) \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# embed the images using the siamese network\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msiameseNetwork((anchor, positive, negative))\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "train_triplets = create_triplets(x_train, y_train, num_triplets=200)\n",
    "test_triplets = create_triplets(x_test, y_test, num_triplets=10)\n",
    "\n",
    "# Prepare the embedding model and siamese network\n",
    "embedding_model = get_embedding_module((28, 28))\n",
    "siamese_network = get_siamese_network((28, 28), embedding_model)\n",
    "\n",
    "# Compile and train the SiameseModel\n",
    "siamese_model = SiameseModel(siamese_network, margin=1, lossTracker=tf.metrics.Mean())\n",
    "siamese_model.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "# Prepare the data\n",
    "anchor_train = train_triplets[:, 0]\n",
    "positive_train = train_triplets[:, 1]\n",
    "negative_train = train_triplets[:, 2]\n",
    "train_data = (anchor_train, positive_train, negative_train)\n",
    "\n",
    "anchor_test = test_triplets[:, 0]\n",
    "positive_test = test_triplets[:, 1]\n",
    "negative_test = test_triplets[:, 2]\n",
    "test_data = (anchor_test, positive_test, negative_test)\n",
    "print(len(train_data))\n",
    "\n",
    "# send only 1 data at a time \n",
    "for _ in range(10):\n",
    "    siamese_model.train_step(train_data[_])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = siamese_model.fit(\n",
    "    train_data,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=test_data\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
